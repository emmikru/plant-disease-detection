


# -- Basis-Imports --
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# -- Machine Learning / Deep Learning --
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import (
    Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
)
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.applications import (
    MobileNetV2, ResNet50V2, EfficientNetB0
)

# -- Tools --
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split



from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

def plot_confusion_matrix(model, test_flow, class_names, title="Konfusionsmatrix"):
    # Testflow zurücksetzen
    test_flow.reset()
    
    # Vorhersagen
    preds = model.predict(test_flow)
    pred_classes = np.argmax(preds, axis=1)

    # Wahre Labels
    true_classes = test_flow.classes

    # Konfusionsmatrix berechnen
    cm = confusion_matrix(true_classes, pred_classes)

    # Plot
    plt.figure(figsize=(12, 10))
    sns.heatmap(
        cm,
        annot=False,
        fmt='d',
        cmap='Blues',
        xticklabels=class_names,
        yticklabels=class_names
    )
    plt.xlabel("Vorhergesagte Klassen")
    plt.ylabel("Wahre Klassen")
    plt.title(title)
    plt.tight_layout()
    plt.show()

    print("Shape der Konfusionsmatrix:", cm.shape)



def show_misclassifications(model, test_flow, test_df, class_names, num_examples=6):
    # Testflow Reset
    test_flow.reset()

    # Vorhersagen
    pred = model.predict(test_flow)
    pred_classes = np.argmax(pred, axis=1)

    # wahre Labels
    true_classes = test_flow.classes

    # Fehler finden
    errors = np.where(pred_classes != true_classes)[0]

    print("Anzahl Fehlklassifikationen:", len(errors))

    for i in errors[:num_examples]:
        img_path = test_df.iloc[i]["path"]

        plt.imshow(plt.imread(img_path))
        plt.title(
            f"True: {class_names[true_classes[i]]}\nPred: {class_names[pred_classes[i]]}",
            fontsize=12
        )
        plt.axis("off")
        plt.show()






import kagglehub

# letzte Version downloaden
path = kagglehub.dataset_download("emmarex/plantdisease")

print("Path to dataset files:", path)





import random
from PIL import Image

# Richtiger PlantVillage-Datenordner
dataset_dir = os.path.join(path, "PlantVillage", "PlantVillage")

print("Datensatzordner:", dataset_dir)

# --- Klassenordner laden ---
folders = [
    f for f in os.listdir(dataset_dir)
    if os.path.isdir(os.path.join(dataset_dir, f))
]

print("Anzahl Klassen:", len(folders))
print("Beispeilklassen:", folders[:10])

# --- Bilder pro Klasse zählen ---
class_counts = {}
total_images = 0 

for cls in folders:
    cls_path = os.path.join(dataset_dir, cls)
    images = [
        f for f in os.listdir(cls_path)
        if f.lower().endswith((".jpg", ".jpeg", ".png"))
    ]
    class_counts[cls] = len(images)
    total_images += len(images)

print("\nBilder pro Klasse:")
for cls, count in class_counts.items():
    print(f"{cls}: {count}")

print(f"\nGesamtanzahl aller Bilder im Datensatz: {total_images}")

# --- Beispielbilder anzeigen ---
sample_classes = random.sample(folders, 4)

plt.figure(figsize=(14, 10))
for i, cls in enumerate(sample_classes):
    cls_path = os.path.join(dataset_dir, cls)
    images = [
        f for f in os.listdir(cls_path)
        if f.lower().endswith((".jpg", ".jpeg", ".png"))
    ]
    
    if not images:
        continue  # überspringt leere Ordner, falls vorhanden
    
    img_path = os.path.join(cls_path, random.choice(images))
    img = Image.open(img_path)

    plt.subplot(2, 2, i + 1)  # 2x2 Layout
    plt.imshow(img)
    plt.title(cls, fontsize=12)
    plt.axis("off")

plt.suptitle("Zufällige Beispielbilder aus verschiedenen Klassen", fontsize=16)
plt.tight_layout()
plt.show()






# Dictionary in DataFrame umwandeln
class_df = pd.DataFrame.from_dict(class_counts, orient='index', columns=['count'])
class_df = class_df.sort_values(by='count', ascending=False)

# Plot
plt.figure(figsize=(14, 6))
plt.bar(class_df.index, class_df['count'], color='steelblue')
plt.xticks(rotation=90)
plt.title("Bilder pro Klasse im PlantVillage-Datensatz", fontsize=16)
plt.xlabel("Kategorien / Krankheiten")
plt.ylabel("Anzahl der Bilder")
plt.tight_layout()
plt.show()






image_paths = []
labels = []

# Durchlaufe jede Klasse und sammle Bildpfade
for cls in folders:
    cls_path = os.path.join(dataset_dir, cls)
    
    images = [
        f for f in os.listdir(cls_path)
        if f.lower().endswith((".jpg", ".jpeg", ".png"))
    ]
    
    for img in images:
        image_paths.append(os.path.join(cls_path, img))
        labels.append(cls)

# In DataFrame überführen
df = pd.DataFrame({
    "path": image_paths,
    "label": labels
})

print(len(df))
df.head()






# --- 1. Split: Train + Temp (Val+Test) ---
train_df, temp_df = train_test_split(
    df,
    test_size=0.30,     # 70% Train, 30% Temp
    stratify=df["label"],
    random_state=42
)

# --- 2. Split: Temp -> Validation + Test ---
val_df, test_df = train_test_split(
    temp_df,
    test_size=0.50,     # Hälfte der 30% -> 15% Test, 15% Val
    stratify=temp_df["label"],
    random_state=42
)

# --- Ausgabe der Größen ---
print("Größe der Datensplits:")
print("Training:", len(train_df))
print("Validation:", len(val_df))
print("Test:", len(test_df))

# --- Prüfung: Anzahl Klassen unverändert? ---
print("\nAnzahl Klassen:")
print("Train:", train_df["label"].nunique())
print("Val:", val_df["label"].nunique())
print("Test:", test_df["label"].nunique())





IMG_SIZE = (128, 128)  # niedriger gestellt, damit Training nicht so lang dauert 
BATCH_SIZE = 32

# --- Trainingsgenerator (mit Augmentation) ---
train_gen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.2,
    horizontal_flip=True
)

# --- Validierungs- & Testgenerator (ohne Augmentation) ---
val_gen = ImageDataGenerator(rescale=1./255)
test_gen = ImageDataGenerator(rescale=1./255)

# --- Generatoren erstellen ---
train_flow = train_gen.flow_from_dataframe(
    train_df,
    x_col="path",
    y_col="label",
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical"
)

val_flow = val_gen.flow_from_dataframe(
    val_df,
    x_col="path",
    y_col="label",
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical"
)

test_flow = test_gen.flow_from_dataframe(
    test_df,
    x_col="path",
    y_col="label",
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    shuffle=False  # Test-Set nie mischen
)






model = Sequential([
    Conv2D(16, (3,3), activation='relu', padding="same", input_shape=(128, 128, 3)),
    MaxPooling2D(2,2),

    Conv2D(32, (3,3), activation='relu', padding='same'),
    MaxPooling2D(2,2),

    Conv2D(64, (3,3), activation='relu', padding='same'),
    MaxPooling2D(2,2),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(len(folders), activation='softmax')
])

model.compile(
    optimizer=Adam(learning_rate=0.0005),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)






early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=2,
    restore_best_weights=True
)





# --- Training ---
history = model.fit(
    train_flow, 
    epochs=5, 
    validation_data=val_flow, 
    callbacks=[early_stopping],
    verbose=1
)





# --- Plots ---
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Accuracy während des Trainings')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss während des Trainings')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()





test_flow.reset()

# Vorhersagen
y_pred = model.predict(test_flow)
y_pred_classes = np.argmax(y_pred, axis=1)

# Wahre Labels
y_true = test_flow.classes

# Klassennamen
class_names = list(test_flow.class_indices.keys())

print("Anzahl der Testbilder:", len(y_true))





print(classification_report(
    y_true, 
    y_pred_classes, 
    target_names=class_names    
))





plot_confusion_matrix(model, test_flow, class_names, title="CNN – Konfusionsmatrix")






from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.layers import GlobalAveragePooling2D
from tensorflow.keras import Model

efficient_train_flow = train_gen.flow_from_dataframe(
    train_df,
    x_col="path",
    y_col="label",
    target_size=(224, 224),     # höher als Baseline, weil Modell höhere Auflöung braucht
    batch_size=BATCH_SIZE,
    class_mode="categorical"
)

efficient_val_flow = val_gen.flow_from_dataframe(
    val_df,
    x_col="path",
    y_col="label",
    target_size=(224, 224),
    batch_size=BATCH_SIZE,
    class_mode="categorical"
)

efficient_test_flow = test_gen.flow_from_dataframe(
    test_df,
    x_col="path",
    y_col="label",
    target_size=(224, 224), 
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    shuffle=False
)

base_model = EfficientNetB0(
    weights="imagenet",
    include_top=False,
    input_shape=(224, 224, 3)  # höhere Bildauflösung, da Modell sonst nicht gut funktioniert 
)
base_model.trainable = False

x = GlobalAveragePooling2D()(base_model.output)
x = Dense(64, activation='relu')(x)
x = Dropout(0.2)(x)
output = Dense(len(folders), activation='softmax')(x)

efficient_model = Model(base_model.input, output)

efficient_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

efficient_model.summary()

history_eff = efficient_model.fit(
    efficient_train_flow,  
    epochs=3, 
    validation_data=efficient_val_flow,
    callbacks=[early_stopping],
    verbose=1
)

eff_val_acc = max(history_eff.history["val_accuracy"])
print("EfficientNetB0 – Beste Validation Accuracy:", eff_val_acc)


plot_confusion_matrix(efficient_model, efficient_test_flow, class_names, title="EfficientNetB0 – Konfusionsmatrix")






from tensorflow.keras.applications import MobileNetV2

# --- Basis-Modell laden ---
base_model_mn = MobileNetV2(
    weights="imagenet",
    include_top=False,
    input_shape=(128, 128, 3)
)

base_model_mn.trainable = False  # Freeze aller Layer

# --- Klassifikationskopf ---
x = GlobalAveragePooling2D()(base_model_mn.output)
x = Dense(64, activation='relu')(x)
x = Dropout(0.2)(x)
output = Dense(len(folders), activation='softmax')(x)

mobilenet_model = Model(base_model_mn.input, output)

mobilenet_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

mobilenet_model.summary()

# --- Training ---
history_mn = mobilenet_model.fit(
    train_flow, 
    epochs=3, 
    validation_data=val_flow, 
    callbacks=[early_stopping],
    verbose=1
)

# --- Beste Accuracy speichern ---
mn_val_acc = max(history_mn.history["val_accuracy"])
print("MobileNetV2 - Beste Validation Accuracy:", mn_val_acc)



plot_confusion_matrix(mobilenet_model, test_flow, class_names, title="MobileNetV2 – Konfusionsmatrix")






from tensorflow.keras.applications import ResNet50V2

# --- Basis-Modell laden ---
base_model_res = ResNet50V2(
    weights="imagenet",
    include_top=False,
    input_shape=(128, 128, 3)
)

base_model_res.trainable = False  # Freeze

# --- Klassifikationskopf ---
x = GlobalAveragePooling2D()(base_model_res.output)
x = Dense(64, activation='relu')(x)
x = Dropout(0.2)(x)
output = Dense(len(folders), activation='softmax')(x)

resnet_model = Model(base_model_res.input, output)

resnet_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

resnet_model.summary()

# --- Training ---
history_res = resnet_model.fit(
    train_flow, 
    epochs=3,
    validation_data=val_flow, 
    callbacks=[early_stopping],
    verbose=1
)

# --- Beste Accuracy speichern ---
res_val_acc = max(history_res.history["val_accuracy"])
print("ResNet50V2 - Beste Validation Accuracy:", res_val_acc)



plot_confusion_matrix(resnet_model, test_flow, class_names, title="ResNet50V2 – Konfusionsmatrix")






results = pd.DataFrame({
    "Model": ["Baseline", "EfficientNetB0", "MobileNetV2", "ResNet50V2"],
    "Val Accuracy": [
        max(history.history["val_accuracy"]),
        max(history_eff.history["val_accuracy"]),
        max(history_mn.history["val_accuracy"]),
        max(history_res.history["val_accuracy"])
    ]
})

print(results)

sns.barplot(data=results, x="Model", y="Val Accuracy")
plt.title("Vergleich der Modelle")
plt.show()






show_misclassifications(model, test_flow, test_df, class_names)



show_misclassifications(efficient_model, efficient_test_flow, test_df, class_names)



show_misclassifications(mobilenet_model, test_flow, test_df, class_names)



show_misclassifications(resnet_model, test_flow, test_df, class_names)




