


# -- Basis-Imports --
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# -- Machine Learning / Deep Learning --
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import (
    Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
)
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.applications import (
    MobileNetV2, ResNet50V2, EfficientNetB0
)

# -- Tools --
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split




def plot_confusion_matrix(model, test_flow, class_names, title="Konfusionsmatrix"):
    # Testflow zurücksetzen
    test_flow.reset()
    
    # Vorhersagen
    preds = model.predict(test_flow)
    pred_classes = np.argmax(preds, axis=1)

    # Wahre Labels
    true_classes = test_flow.classes

    # Konfusionsmatrix berechnen
    cm = confusion_matrix(true_classes, pred_classes)

    # Plot
    plt.figure(figsize=(12, 10))
    sns.heatmap(
        cm,
        annot=False,
        fmt='d',
        cmap='Blues',
        xticklabels=class_names,
        yticklabels=class_names
    )
    plt.xlabel("Vorhergesagte Klassen")
    plt.ylabel("Wahre Klassen")
    plt.title(title)
    plt.tight_layout()
    plt.show()

    print("Shape der Konfusionsmatrix:", cm.shape)


def show_misclassifications(model, test_flow, test_df, class_names, num_examples=6):
    # Testflow Reset
    test_flow.reset()

    # Vorhersagen
    pred = model.predict(test_flow)
    pred_classes = np.argmax(pred, axis=1)

    # wahre Labels
    true_classes = test_flow.classes

    # Fehler finden
    errors = np.where(pred_classes != true_classes)[0]

    print("Anzahl Fehlklassifikationen:", len(errors))

    for i in errors[:num_examples]:
        img_path = test_df.iloc[i]["path"]

        plt.imshow(plt.imread(img_path))
        plt.title(
            f"True: {class_names[true_classes[i]]}\nPred: {class_names[pred_classes[i]]}",
            fontsize=12
        )
        plt.axis("off")
        plt.show()






import kagglehub

# letzte Version downloaden
path = kagglehub.dataset_download("emmarex/plantdisease")

print("Path to dataset files:", path)





import random
from PIL import Image

# Richtiger PlantVillage-Datenordner
dataset_dir = os.path.join(path, "PlantVillage", "PlantVillage")

print("Datensatzordner:", dataset_dir)

# --- Klassenordner laden ---
folders = [
    f for f in os.listdir(dataset_dir)
    if os.path.isdir(os.path.join(dataset_dir, f))
]

print("Anzahl Klassen:", len(folders))
print("Beispeilklassen:", folders[:10])

# --- Bilder pro Klasse zählen ---
class_counts = {}
total_images = 0 

for cls in folders:
    cls_path = os.path.join(dataset_dir, cls)
    images = [
        f for f in os.listdir(cls_path)
        if f.lower().endswith((".jpg", ".jpeg", ".png"))
    ]
    class_counts[cls] = len(images)
    total_images += len(images)

print("\nBilder pro Klasse:")
for cls, count in class_counts.items():
    print(f"{cls}: {count}")

print(f"\nGesamtanzahl aller Bilder im Datensatz: {total_images}")

# --- Beispielbilder anzeigen ---
sample_classes = random.sample(folders, 4)

plt.figure(figsize=(14, 10))
for i, cls in enumerate(sample_classes):
    cls_path = os.path.join(dataset_dir, cls)
    images = [
        f for f in os.listdir(cls_path)
        if f.lower().endswith((".jpg", ".jpeg", ".png"))
    ]
    
    if not images:
        continue  # überspringt leere Ordner, falls vorhanden
    
    img_path = os.path.join(cls_path, random.choice(images))
    img = Image.open(img_path)

    plt.subplot(2, 2, i + 1)  # 2x2 Layout
    plt.imshow(img)
    plt.title(cls, fontsize=12)
    plt.axis("off")

plt.suptitle("Zufällige Beispielbilder aus verschiedenen Klassen", fontsize=16)
plt.tight_layout()
plt.show()






# Dictionary in DataFrame umwandeln
class_df = pd.DataFrame.from_dict(class_counts, orient='index', columns=['count'])
class_df = class_df.sort_values(by='count', ascending=False)

# Plot
plt.figure(figsize=(14, 6))
plt.bar(class_df.index, class_df['count'], color='steelblue')
plt.xticks(rotation=90)
plt.title("Bilder pro Klasse im PlantVillage-Datensatz", fontsize=16)
plt.xlabel("Kategorien / Krankheiten")
plt.ylabel("Anzahl der Bilder")
plt.tight_layout()
plt.show()






image_paths = []
labels = []

# Durchlaufe jede Klasse und sammle Bildpfade
for cls in folders:
    cls_path = os.path.join(dataset_dir, cls)
    
    images = [
        f for f in os.listdir(cls_path)
        if f.lower().endswith((".jpg", ".jpeg", ".png"))
    ]
    
    for img in images:
        image_paths.append(os.path.join(cls_path, img))
        labels.append(cls)

# In DataFrame überführen
df = pd.DataFrame({
    "path": image_paths,
    "label": labels
})

print(len(df))
df.head()






# --- 1. Split: Train + Temp (Val+Test) ---
train_df, temp_df = train_test_split(
    df,
    test_size=0.30,     # 70% Train, 30% Temp
    stratify=df["label"],
    random_state=42
)

# --- 2. Split: Temp -> Validation + Test ---
val_df, test_df = train_test_split(
    temp_df,
    test_size=0.50,     # Hälfte der 30% -> 15% Test, 15% Val
    stratify=temp_df["label"],
    random_state=42
)

# --- Ausgabe der Größen ---
print("Größe der Datensplits:")
print("Training:", len(train_df))
print("Validation:", len(val_df))
print("Test:", len(test_df))

# --- Prüfung: Anzahl Klassen unverändert? ---
print("\nAnzahl Klassen:")
print("Train:", train_df["label"].nunique())
print("Val:", val_df["label"].nunique())
print("Test:", test_df["label"].nunique())



# Klassenreihenfolge EINMAL festlegen
class_names = sorted(train_df["label"].unique())

print("Klassenreihenfolge:")
for i, cls in enumerate(class_names):
    print(i, cls)





IMG_SIZE = (128, 128)
BATCH_SIZE = 32

# --- Trainingsgenerator (mit Augmentation) ---
train_gen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.2,
    horizontal_flip=True
)

# --- Validierungs- & Testgenerator (ohne Augmentation) ---
val_gen = ImageDataGenerator(rescale=1./255)
test_gen = ImageDataGenerator(rescale=1./255)

# --- Generatoren erstellen ---
train_flow = train_gen.flow_from_dataframe(
    train_df,
    x_col="path",
    y_col="label",
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical"
)

val_flow = val_gen.flow_from_dataframe(
    val_df,
    x_col="path",
    y_col="label",
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical"
)

test_flow = test_gen.flow_from_dataframe(
    test_df,
    x_col="path",
    y_col="label",
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    shuffle=False  # Test-Set nie mischen
)






model = Sequential([
    Conv2D(16, (3,3), activation='relu', padding="same", input_shape=(128, 128, 3)),
    MaxPooling2D(2,2),

    Conv2D(32, (3,3), activation='relu', padding='same'),
    MaxPooling2D(2,2),

    Conv2D(64, (3,3), activation='relu', padding='same'),
    MaxPooling2D(2,2),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(len(class_names), activation='softmax')
])

model.compile(
    optimizer=Adam(learning_rate=0.0005),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)






early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=2,
    restore_best_weights=True
)





# --- Training ---
history = model.fit(
    train_flow, 
    epochs=5, 
    validation_data=val_flow, 
    callbacks=[early_stopping],
    verbose=1
)





# --- Plots ---
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Accuracy während des Trainings')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Loss während des Trainings')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()





test_flow.reset()

# Vorhersagen
y_pred = model.predict(test_flow)
y_pred_classes = np.argmax(y_pred, axis=1)

# Wahre Labels
y_true = test_flow.classes

# Klassennamen
class_names = list(test_flow.class_indices.keys())

print("Anzahl der Testbilder:", len(y_true))





print(classification_report(
    y_true, 
    y_pred_classes, 
    target_names=class_names    
))





plot_confusion_matrix(model, test_flow, class_names, title="CNN – Konfusionsmatrix")






from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.layers import GlobalAveragePooling2D
from tensorflow.keras import Model
from tensorflow.keras.applications.efficientnet import preprocess_input

efficient_train_gen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    rotation_range=20,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.2,
    horizontal_flip=True
)

efficient_val_gen = ImageDataGenerator(
    preprocessing_function=preprocess_input
)

efficient_test_gen = ImageDataGenerator(
    preprocessing_function=preprocess_input
)

efficient_train_flow = efficient_train_gen.flow_from_dataframe(
    train_df,
    x_col="path",
    y_col="label",
    classes=class_names,
    target_size=(224, 224),
    batch_size=BATCH_SIZE,
    class_mode="categorical"
)

efficient_val_flow = efficient_val_gen.flow_from_dataframe(
    val_df,
    x_col="path",
    y_col="label",
    classes=class_names, 
    target_size=(224, 224),
    batch_size=BATCH_SIZE,
    class_mode="categorical"
)

efficient_test_flow = efficient_test_gen.flow_from_dataframe(
    test_df,
    x_col="path",
    y_col="label",
    classes=class_names, 
    target_size=(224, 224), 
    batch_size=BATCH_SIZE,
    class_mode="categorical",
    shuffle=False
)

base_model = EfficientNetB0(
    weights="imagenet",
    include_top=False,
    input_shape=(224, 224, 3)  # höhere Bildauflösung, da Modell sonst nicht gut funktioniert 
)
base_model.trainable = False

x = GlobalAveragePooling2D()(base_model.output)
x = Dense(64, activation='relu')(x)
x = Dropout(0.2)(x)
output = Dense(len(class_names), activation='softmax')(x)

efficient_model = Model(base_model.input, output)

efficient_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

efficient_model.summary()

history_eff = efficient_model.fit(
    efficient_train_flow,  
    epochs=3, 
    validation_data=efficient_val_flow,
    callbacks=[early_stopping],
    verbose=1
)

eff_val_acc = max(history_eff.history["val_accuracy"])
print("EfficientNetB0 – Beste Validation Accuracy:", eff_val_acc)


plot_confusion_matrix(efficient_model, efficient_test_flow, class_names, title="EfficientNetB0 – Konfusionsmatrix")






from tensorflow.keras.applications import MobileNetV2

# --- Basis-Modell laden ---
base_model_mn = MobileNetV2(
    weights="imagenet",
    include_top=False,
    input_shape=(128, 128, 3)
)

base_model_mn.trainable = False  # Freeze aller Layer

# --- Klassifikationskopf ---
x = GlobalAveragePooling2D()(base_model_mn.output)
x = Dense(64, activation='relu')(x)
x = Dropout(0.2)(x)
output = Dense(len(class_names), activation='softmax')(x)

mobilenet_model = Model(base_model_mn.input, output)

mobilenet_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

mobilenet_model.summary()

# --- Training ---
history_mn = mobilenet_model.fit(
    train_flow, 
    epochs=3, 
    validation_data=val_flow, 
    callbacks=[early_stopping],
    verbose=1
)

# --- Beste Accuracy speichern ---
mn_val_acc = max(history_mn.history["val_accuracy"])
print("MobileNetV2 - Beste Validation Accuracy:", mn_val_acc)



plot_confusion_matrix(mobilenet_model, test_flow, class_names, title="MobileNetV2 – Konfusionsmatrix")






from tensorflow.keras.applications import ResNet50V2

# --- Basis-Modell laden ---
base_model_res = ResNet50V2(
    weights="imagenet",
    include_top=False,
    input_shape=(128, 128, 3)
)

base_model_res.trainable = False  # Freeze

# --- Klassifikationskopf ---
x = GlobalAveragePooling2D()(base_model_res.output)
x = Dense(64, activation='relu')(x)
x = Dropout(0.2)(x)
output = Dense(len(class_names), activation='softmax')(x)

resnet_model = Model(base_model_res.input, output)

resnet_model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

resnet_model.summary()

# --- Training ---
history_res = resnet_model.fit(
    train_flow, 
    epochs=3,
    validation_data=val_flow, 
    callbacks=[early_stopping],
    verbose=1
)

# --- Beste Accuracy speichern ---
res_val_acc = max(history_res.history["val_accuracy"])
print("ResNet50V2 - Beste Validation Accuracy:", res_val_acc)



plot_confusion_matrix(resnet_model, test_flow, class_names, title="ResNet50V2 – Konfusionsmatrix")






results = pd.DataFrame({
    "Model": ["Baseline", "EfficientNetB0", "MobileNetV2", "ResNet50V2"],
    "Val Accuracy": [
        max(history.history["val_accuracy"]),
        max(history_eff.history["val_accuracy"]),
        max(history_mn.history["val_accuracy"]),
        max(history_res.history["val_accuracy"])
    ]
})

print(results)

sns.barplot(data=results, x="Model", y="Val Accuracy")
plt.title("Vergleich der Modelle")
plt.show()






show_misclassifications(model, test_flow, test_df, class_names)



show_misclassifications(efficient_model, efficient_test_flow, test_df, class_names)



show_misclassifications(mobilenet_model, test_flow, test_df, class_names)



show_misclassifications(resnet_model, test_flow, test_df, class_names)






def find_last_conv_layer(model):
    # Sucht die letzte Conv2D-Schicht im Modell
    for layer in reversed(model.layers):
        if isinstance(layer, tf.keras.layers.Conv2D):
            return layer.name
        
    for layer in reversed(model.layers):
        if isinstance(layer, tf.keras.Model):
            for sublayer in reversed(layer.layers):
                if isinstance(sublayer, tf.keras.layers.Conv2D):
                    return sublayer.name
                
    raise ValueError("Keine Conv2D-Schicht gefunden.")

def make_gradcam_heatmap(img_array, model, pred_index=None):
    """
    Robust für tf.keras.Sequential (Baseline-CNN):
    - nutzt model.inputs/output (über funktionalen Call)
    - findet die letzte Conv2D-Schicht im Sequential
    - baut ein grad_model mit conv_output + predictions
    """
    # Modell "callen" -> funktionaler Graph
    inputs = tf.keras.Input(shape=img_array.shape[1:])
    x = inputs
    last_conv = None

    # Forward durch die Layer, letzte Conv merken
    for layer in model.layers:
        x = layer(x)
        if isinstance(layer, tf.keras.layers.Conv2D):
            last_conv = x

    if last_conv is None:
        raise ValueError("Keine Conv2D-Schicht im Sequential gefunden.")

    outputs = x  # softmax output
    grad_model = Model(inputs, [last_conv, outputs])

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array, training=False)
        if pred_index is None:
            pred_index = int(tf.argmax(predictions[0]))
        class_channel = predictions[:, pred_index]

    grads = tape.gradient(class_channel, conv_outputs)
    if grads is None:
        raise RuntimeError("Gradienten sind None – Grad-CAM konnte nicht berechnet werden.")

    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
    conv_outputs = conv_outputs[0]

    heatmap = tf.reduce_sum(pooled_grads * conv_outputs, axis=-1)
    heatmap = tf.maximum(heatmap, 0)
    heatmap = heatmap / (tf.reduce_max(heatmap) + 1e-8)
    return heatmap.numpy()



def load_and_preprocess_image(img_path, target_size):
    img = tf.keras.preprocessing.image.load_img(img_path, target_size=target_size)
    img_array = tf.keras.preprocessing.image.img_to_array(img)
    img_array = img_array / 255.0  # Normalisierung
    return img, np.expand_dims(img_array, axis=0)

def show_gradcam_on_image(img_path, model, class_names, target_size, last_conv_layer_name=None):
    img, img_array = load_and_preprocess_image(img_path, target_size)

    preds = model.predict(img_array, verbose=0)[0]
    pred_class = int(np.argmax(preds))
    pred_label = class_names[pred_class]
    pred_conf = float(np.max(preds))

    heatmap = make_gradcam_heatmap(img_array, model, pred_index=pred_class)
    heatmap_resized = tf.image.resize(heatmap[..., np.newaxis], target_size).numpy().squeeze()
    
    plt.figure(figsize=(10, 4))

    plt.subplot(1, 3, 1)
    plt.imshow(img)
    plt.title("Original Image")
    plt.axis("off")

    plt.subplot(1, 3, 2)
    plt.imshow(img)
    plt.imshow(heatmap_resized, alpha=0.4)
    plt.title("Grad-CAM Overlay")
    plt.axis("off")

    plt.subplot(1, 3, 3)
    plt.imshow(heatmap_resized)
    plt.title("Grad-CAM Heatmap")
    plt.axis("off")

    plt.suptitle(f"Predicted: {pred_label} ({pred_conf:.2f})", fontsize=12)
    plt.tight_layout()
    plt.show()



# Modell "bauen" / einmal aufrufen, damit model.output existiert
dummy = np.zeros((1, IMG_SIZE[0], IMG_SIZE[1], 3), dtype=np.float32)
_ = model.predict(dummy, verbose=0)

print("Model ist jetzt gebaut. Output shape:", model.output_shape)



sample_paths = test_df.sample(2, random_state=42)["path"].to_list()

for path in sample_paths:
    show_gradcam_on_image(
        img_path=path,
        model=model,
        class_names=class_names,
        target_size=IMG_SIZE
    )



